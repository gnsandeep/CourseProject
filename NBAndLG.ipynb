{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterdata = pd.read_csv(\"data/dataPandas.csv\")\n",
    "twitterdata = twitterdata.reindex(np.random.permutation(twitterdata.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', x))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        df = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "tc = TextCounts()\n",
    "twitterdata_eda = tc.fit_transform(twitterdata.Tweets)\n",
    "twitterdata_eda['Labels'] = twitterdata.Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)    \n",
    "   \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "sr_clean_td = ct.fit_transform(twitterdata.Tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n"
     ]
    }
   ],
   "source": [
    "empty_clean = sr_clean_td == ''\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean_td[empty_clean].count()))\n",
    "sr_clean_td.loc[empty_clean] = '[no_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ = CountVectorizer()\n",
    "bow = cv_.fit_transform(sr_clean_td)\n",
    "#word_freq = dict(zip(cv_.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
    "#word_counter = collections.Counter(word_freq)\n",
    "#word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count_words', 'count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls', 'count_emojis', 'Labels', 'clean_text']\n"
     ]
    }
   ],
   "source": [
    "df_model_td = twitterdata_eda\n",
    "df_model_td['clean_text'] = sr_clean_td\n",
    "print(df_model_td.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols   \n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(df_model_td.drop('Labels', axis=1), df_model_td.Labels, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_vect1(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    #textcountscols = ['count_emojis','count_words']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))]\n",
    "                                , n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                                , n_jobs=-1)    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)    \n",
    "    t0 = time()\n",
    "    print(X_train.columns.tolist())\n",
    "    grid_search.fit(X_train, y_train1)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()    \n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test1))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test1, grid_search.best_estimator_.predict(X_test)))\n",
    "                        \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': ( 0.25, 0.5, 0.75, 0.9),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2) ,(2,2)),\n",
    "    'features__pipe__vect__min_df': (1,2),\n",
    "    'features__pipe__vect__sublinear_tf' : (True , False)\n",
    "}\n",
    "\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75)\n",
    "}\n",
    "\n",
    "#\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25,0.5,1.0,2.0,3.0,4.0,5.0,6.0,7.0),\n",
    "    'clf__penalty': ('l1', 'l2')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0),\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75, 0.9),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2), (2, 2)),\n",
      " 'features__pipe__vect__sublinear_tf': (True, False)}\n",
      "['count_words', 'count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls', 'count_emojis', 'clean_text']\n",
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1555.039s\n",
      "\n",
      "Best CV score: 0.775\n",
      "Best parameters set:\n",
      "\tclf__C: 3.0\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe__vect__max_df: 0.5\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe__vect__sublinear_tf: True\n",
      "Test score with best_estimator_: 0.776\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NOT_SARCASM       0.80      0.73      0.76       249\n",
      "     SARCASM       0.75      0.82      0.79       251\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       500\n",
      "   macro avg       0.78      0.78      0.78       500\n",
      "weighted avg       0.78      0.78      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer()# MultinomialNB\n",
    "#best_mnb_tfidf = grid_vect1(mnb, parameters_mnb, X_train1, X_test1, parameters_text=parameters_vect, vect=tfidfvect)\n",
    "#joblib.dump(best_mnb_tfidf, '../output/best_mnb_tfidf.pkl')# LogisticRegression\n",
    "best_logreg_tfidf = grid_vect1(logreg, parameters_logreg, X_train1, X_test1, parameters_text=parameters_vect, vect=tfidfvect)\n",
    "#joblib.dump(best_logreg_tfidf, '../output/best_logreg_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    ",'count_mentions','count_urls','count_words']\n",
    "\n",
    "features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    ", ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))\n",
    ", ('vect', TfidfVectorizer(max_df=0.75, min_df=2, ngram_range=(1,2) , sublinear_tf= True))]))]\n",
    ", n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([('features', features)\n",
    ", ('clf', LogisticRegression(C=1, penalty='l2'))\n",
    "])\n",
    "best_model = pipeline.fit(df_model_td.drop('Labels', axis=1), df_model_td.Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata = pd.read_csv(\"data/dftestdata.csv\")\n",
    "#twittertestdata.Tweets = twittertestdata['Tweets'].apply(preprocess_tweet_text)\n",
    "\n",
    "#tf_vector_test = get_feature_vector(np.array(twittertestdata.iloc[:, 1]).ravel())\n",
    "#X_testing = tf_vector.transform(np.array(twittertestdata.iloc[:, 1]).ravel())\n",
    "\n",
    "df_counts_pos = tc.transform(twittertestdata['Tweets'])\n",
    "df_clean_pos = ct.transform(twittertestdata['Tweets'])\n",
    "df_model_pos = df_counts_pos\n",
    "df_model_pos['clean_text'] = df_clean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction = best_model.predict(df_model_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata['PredictLG'] = best_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata.to_csv('answerCVLG.txt', columns = [\"ID\" , \"PredictLG\"] , header = False , index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
