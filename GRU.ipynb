{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import sklearn\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM,Dropout , Input, GRU\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import SGD\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau ,EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data_to_pandas(filename = 'data/train.jsonl'):\n",
    "    X = []\n",
    "    Y = []\n",
    "    fhand = open(filename,encoding='utf8')\n",
    "    for line in fhand:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        lt = data['context']\n",
    "        lt.reverse()\n",
    "        fullTweet =   data['response'] + \" \" + ''.join(lt)\n",
    "\n",
    "        X.append(fullTweet)\n",
    "        Y.append(data['label'])\n",
    " \n",
    "    \n",
    "    dfdata = pd.DataFrame({'Tweets': X,'Labels': Y}) \n",
    "\n",
    "    dfdata.to_csv(r'data/dataPandas.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_to_pandas(filename = 'data/test.jsonl'):\n",
    "    tid = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    fhand = open(filename,encoding='utf8')\n",
    "    for line in fhand:\n",
    "        data = json.loads(line)\n",
    "        tid.append(data['id'])\n",
    "        lt = data['context']\n",
    "        lt.reverse()\n",
    "        fullTweet =   data['response'] + \" \" + ''.join(lt)\n",
    "\n",
    "        X.append(fullTweet)\n",
    "        \n",
    "    \n",
    "    dftestdata = pd.DataFrame({'ID': tid,\n",
    "                   'Tweets': X})\n",
    "    \n",
    "   \n",
    "    dftestdata.to_csv(r'data/dftestdata.csv',index=False)\n",
    "\n",
    "\n",
    "    #return X_train,Y_train,X_test,Y_test,maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_data_to_pandas()\n",
    "load_test_data_to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterdata = pd.read_csv(\"data/dataPandas.csv\")\n",
    "twitterdata.isnull().values.any()\n",
    "twitterdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d845ae4278>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Labels', data=twitterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = sen.replace('@USER',' ')\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    words = sen.split() \n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    sentence = \" \".join(stemmed_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(twitterdata['Tweets'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = twitterdata['Labels']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"SARCASM\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('./Data/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "#glove_file = open('./Data/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 100)          905600    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 915,601\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 905,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 200, 100)          2263300   \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 200, 128)          88320     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_20 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,353,701\n",
      "Trainable params: 2,353,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#GRU\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=True)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(128,dropout=0.5, recurrent_dropout=0.2,return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience =2)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7059 - acc: 0.5061\n",
      "Epoch 00001: val_acc improved from -inf to 0.63583, saving model to best_model.h5\n",
      "22/22 [==============================] - 32s 1s/step - loss: 0.7059 - acc: 0.5061 - val_loss: 0.6837 - val_acc: 0.6358\n",
      "Epoch 2/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6825 - acc: 0.5446\n",
      "Epoch 00002: val_acc did not improve from 0.63583\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.6825 - acc: 0.5446 - val_loss: 0.6698 - val_acc: 0.6125\n",
      "Epoch 3/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6619 - acc: 0.6025\n",
      "Epoch 00003: val_acc did not improve from 0.63583\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.6619 - acc: 0.6025 - val_loss: 0.6526 - val_acc: 0.6000\n",
      "Epoch 4/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6382 - acc: 0.6268\n",
      "Epoch 00004: val_acc improved from 0.63583 to 0.71333, saving model to best_model.h5\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6382 - acc: 0.6268 - val_loss: 0.5873 - val_acc: 0.7133\n",
      "Epoch 5/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6029 - acc: 0.6700\n",
      "Epoch 00005: val_acc improved from 0.71333 to 0.71583, saving model to best_model.h5\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6029 - acc: 0.6700 - val_loss: 0.5698 - val_acc: 0.7158\n",
      "Epoch 6/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5777 - acc: 0.6993\n",
      "Epoch 00006: val_acc improved from 0.71583 to 0.74083, saving model to best_model.h5\n",
      "22/22 [==============================] - 36s 2s/step - loss: 0.5777 - acc: 0.6993 - val_loss: 0.5346 - val_acc: 0.7408\n",
      "Epoch 7/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5643 - acc: 0.7250\n",
      "Epoch 00007: val_acc did not improve from 0.74083\n",
      "22/22 [==============================] - 36s 2s/step - loss: 0.5643 - acc: 0.7250 - val_loss: 0.5337 - val_acc: 0.7258\n",
      "Epoch 8/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5433 - acc: 0.7361\n",
      "Epoch 00008: val_acc did not improve from 0.74083\n",
      "22/22 [==============================] - 36s 2s/step - loss: 0.5433 - acc: 0.7361 - val_loss: 0.5429 - val_acc: 0.7183\n",
      "Epoch 9/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5393 - acc: 0.7311\n",
      "Epoch 00009: val_acc did not improve from 0.74083\n",
      "22/22 [==============================] - 37s 2s/step - loss: 0.5393 - acc: 0.7311 - val_loss: 0.5182 - val_acc: 0.7383\n",
      "Epoch 10/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5060 - acc: 0.7586\n",
      "Epoch 00010: val_acc did not improve from 0.74083\n",
      "22/22 [==============================] - 37s 2s/step - loss: 0.5060 - acc: 0.7586 - val_loss: 0.5498 - val_acc: 0.7350\n",
      "Epoch 11/25\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5023 - acc: 0.7689\n",
      "Epoch 00011: val_acc did not improve from 0.74083\n",
      "22/22 [==============================] - 39s 2s/step - loss: 0.5023 - acc: 0.7689 - val_loss: 0.6135 - val_acc: 0.7000\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, epochs=25, verbose=1, validation_split=0.3, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 162ms/step - loss: 0.5563 - acc: 0.7230\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')\n",
    "score,accuracy = saved_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 160ms/step - loss: 0.5563 - acc: 0.7230\n"
     ]
    }
   ],
   "source": [
    "score,accuracy = saved_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.5562872290611267\n",
      "Test Accuracy: 0.7229999899864197\n",
      "1000\n",
      "[0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
      " 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
      " 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
      " 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
      " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
      " 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
      " 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1\n",
      " 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
      " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0\n",
      " 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0\n",
      " 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1\n",
      " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1\n",
      " 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 0]\n",
      "[[302 198]\n",
      " [ 79 421]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score:\", score)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "y_pred = saved_model.predict(X_test)\n",
    "y_pred =(y_pred>0.5)\n",
    "print(len(y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.604, 0.7926509186351706, 0.6855845629965948)\n"
     ]
    }
   ],
   "source": [
    "def myscores(smat): \n",
    "    tp = smat[0][0] \n",
    "    fp = smat[0][1] \n",
    "    fn = smat[1][0] \n",
    "    tn = smat[1][1] \n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "    f1 = (2*p*r)/(p+r)\n",
    "    return p,r,f1\n",
    "\n",
    "print(myscores(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twittertestdata = pd.read_csv(\"data/dftestdata.csv\")\n",
    "twittertestdata.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "sentences = list(twittertestdata['Tweets'])\n",
    "for sen in sentences:\n",
    "    X_val.append(preprocess_text(sen))\n",
    "    \n",
    "X_validate = tokenizer.texts_to_sequences(X_val)\n",
    "#X_valTokens = tokenizer.texts_to_sequences(X_validate)\n",
    "X_valTokens = pad_sequences(X_validate, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = model.predict(X_valTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata['Predict'] = validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata['PLabel'] = np.where(twittertestdata['Predict'] > 0.5, \"SARCASM\", \"NOT_SARCASM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Predict</th>\n",
       "      <th>PLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter_1</td>\n",
       "      <td>@USER @USER @USER My 3 year old , that just fi...</td>\n",
       "      <td>0.886082</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter_2</td>\n",
       "      <td>@USER @USER How many verifiable lies has he to...</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter_3</td>\n",
       "      <td>@USER @USER @USER Maybe Docs just a scrub of a...</td>\n",
       "      <td>0.858443</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter_4</td>\n",
       "      <td>@USER @USER is just a cover up for the real ha...</td>\n",
       "      <td>0.828586</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter_5</td>\n",
       "      <td>@USER @USER @USER The irony being that he even...</td>\n",
       "      <td>0.938814</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             Tweets   Predict  \\\n",
       "0  twitter_1  @USER @USER @USER My 3 year old , that just fi...  0.886082   \n",
       "1  twitter_2  @USER @USER How many verifiable lies has he to...  0.902439   \n",
       "2  twitter_3  @USER @USER @USER Maybe Docs just a scrub of a...  0.858443   \n",
       "3  twitter_4  @USER @USER is just a cover up for the real ha...  0.828586   \n",
       "4  twitter_5  @USER @USER @USER The irony being that he even...  0.938814   \n",
       "\n",
       "    PLabel  \n",
       "0  SARCASM  \n",
       "1  SARCASM  \n",
       "2  SARCASM  \n",
       "3  SARCASM  \n",
       "4  SARCASM  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twittertestdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata.to_csv('answer_KerasLSTM.txt', columns = [\"ID\" , \"PLabel\"] , header = False , index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
